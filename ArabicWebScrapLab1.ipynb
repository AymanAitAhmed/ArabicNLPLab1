{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### MST: IASD, NLP: LAB 1\n",
    "this notebook describes the steps of scraping an arabic sports website for articles, inserting data in MongoDB, establishing of NLP pipeline and using different NLP technics such us stemming, lemmetization, PoS and NER.\n",
    "this work is done by **Aymen Ait Ahmed**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ae29975cee21ca7"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-04T16:13:27.763698900Z",
     "start_time": "2024-04-04T16:13:27.555100Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests as rs"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Scraping www.almayadeen.net/all-sports using BeautifulSoup library"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a22516e32d8d728"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "url = \"https://www.almayadeen.net/all-sports\"\n",
    "\n",
    "soup = BeautifulSoup(rs.get(url).content.decode('UTF-8'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T16:13:30.460572200Z",
     "start_time": "2024-04-04T16:13:28.409252Z"
    }
   },
   "id": "a15a9ef83460559a"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "articles = []\n",
    "for i in soup.find_all('div', {'class', 'grid-item'}):\n",
    "    try:\n",
    "        article_html = i.find('div', {'class', 'grid-title fixed-height'})\n",
    "        temp = str(article_html.a['href'].encode('utf8')).replace(r'\\x', '%').replace('/sports',\n",
    "                                                                                      '').upper().removeprefix(\n",
    "            \"B'\").removesuffix(\"'\")\n",
    "        article_link = 'https://www.almayadeen.net/sports' + temp\n",
    "        article_title = article_html.a.h4.text\n",
    "        temp_soup = BeautifulSoup(rs.get(article_link).text)\n",
    "        article_content = ''.join(\n",
    "            [i.text for i in temp_soup.find('div', {'class', 'p-content'}).find_all('p', recursive=False)])\n",
    "        article = {'link': article_link, 'title': article_title, 'content': article_content}\n",
    "        articles.append(article)\n",
    "    except:\n",
    "        continue"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T16:14:05.963510700Z",
     "start_time": "2024-04-04T16:13:31.298601500Z"
    }
   },
   "id": "39aa96b1c8680f89"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "12"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T16:14:08.237951200Z",
     "start_time": "2024-04-04T16:14:08.176830300Z"
    }
   },
   "id": "bec2f27cfb8f5b16"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inserting data in MongoDB"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3409b1b95da84281"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymongo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Obtaining dependency information for pymongo from https://files.pythonhosted.org/packages/6b/38/11db8be24e20b0885182aaa3839db2eb2be51f506a222538762e1b3723f5/pymongo-4.6.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pymongo-4.6.3-cp311-cp311-win_amd64.whl.metadata (22 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
      "  Obtaining dependency information for dnspython<3.0.0,>=1.16.0 from https://files.pythonhosted.org/packages/87/a1/8c5287991ddb8d3e4662f71356d9656d91ab3a36618c3dd11b280df0d255/dnspython-2.6.1-py3-none-any.whl.metadata\n",
      "  Downloading dnspython-2.6.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Downloading pymongo-4.6.3-cp311-cp311-win_amd64.whl (472 kB)\n",
      "   ---------------------------------------- 0.0/472.9 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 41.0/472.9 kB 960.0 kB/s eta 0:00:01\n",
      "   ----- --------------------------------- 61.4/472.9 kB 648.1 kB/s eta 0:00:01\n",
      "   --------- ------------------------------ 112.6/472.9 kB 1.1 MB/s eta 0:00:01\n",
      "   --------- ---------------------------- 122.9/472.9 kB 654.9 kB/s eta 0:00:01\n",
      "   ------------------- -------------------- 235.5/472.9 kB 1.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 286.7/472.9 kB 1.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 472.9/472.9 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
      "   ---------------------------------------- 0.0/307.7 kB ? eta -:--:--\n",
      "   ------------------------------ --------- 235.5/307.7 kB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 307.7/307.7 kB 4.7 MB/s eta 0:00:00\n",
      "Installing collected packages: dnspython, pymongo\n",
      "Successfully installed dnspython-2.6.1 pymongo-4.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T18:26:37.699216900Z",
     "start_time": "2024-04-02T18:26:25.244719600Z"
    }
   },
   "id": "609dcc7fc4463d91"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "mongo_client = pymongo.MongoClient('mongodb://127.0.0.1:27017')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:53:59.049649400Z",
     "start_time": "2024-04-02T19:53:58.028316900Z"
    }
   },
   "id": "da532cb47bc96ef0"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "db = mongo_client['articles']\n",
    "db_collection = db.sportArticles"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:55:37.056185800Z",
     "start_time": "2024-04-02T19:55:37.011185900Z"
    }
   },
   "id": "4d165466bd67e81"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "InsertManyResult([ObjectId('660c63115408d82d214bf57b'), ObjectId('660c63115408d82d214bf57c'), ObjectId('660c63115408d82d214bf57d'), ObjectId('660c63115408d82d214bf57e'), ObjectId('660c63115408d82d214bf57f'), ObjectId('660c63115408d82d214bf580'), ObjectId('660c63115408d82d214bf581'), ObjectId('660c63115408d82d214bf582'), ObjectId('660c63115408d82d214bf583'), ObjectId('660c63115408d82d214bf584'), ObjectId('660c63115408d82d214bf585'), ObjectId('660c63115408d82d214bf586')], acknowledged=True)"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_collection.insert_many(articles)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T19:57:05.065325800Z",
     "start_time": "2024-04-02T19:57:05.052291900Z"
    }
   },
   "id": "cc8e1c2ef95275a0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Establishment of NLP Pipeline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "daa03546d59efe32"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Obtaining dependency information for nltk from https://files.pythonhosted.org/packages/a6/0a/0d20d2c0f16be91b9fa32a77b76c60f9baf6eba419e5ef5deca17af9c582/nltk-3.8.1-py3-none-any.whl.metadata\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting click (from nltk)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\acer\\pycharmprojects\\logisticregression\\env\\python311\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/a8/01/18232f93672c1d530834e2e0568a80eaab1df12d67ae499b1762ab462b5c/regex-2023.12.25-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ----------------------------- ---------- 30.7/42.0 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 42.0/42.0 kB 675.2 kB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/2a/14/e75e52d521442e2fcc9f1df3c5e456aead034203d4797867980de558ab34/tqdm-4.66.2-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\pycharmprojects\\logisticregression\\env\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "   --- ------------------------------------ 0.1/1.5 MB 1.4 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.5 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 0.7/1.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.9/1.5 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.2/1.5 MB 3.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.2/1.5 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.5 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 174.1/269.5 kB 5.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 269.5/269.5 kB 4.1 MB/s eta 0:00:00\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1 regex-2023.12.25 tqdm-4.66.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:07:53.279982100Z",
     "start_time": "2024-04-02T20:07:36.526564900Z"
    }
   },
   "id": "e29c2db7700e1506"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:46:52.668426600Z",
     "start_time": "2024-04-02T20:46:44.654861500Z"
    }
   },
   "id": "9cf17c054ef55e00"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tokenization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97ed4c21a8927918"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_titles = []\n",
    "tokenized_contents = []\n",
    "len_before = 0\n",
    "for article in articles:\n",
    "    tokenized_title = word_tokenize(article['title'])\n",
    "    len_before += len(tokenized_title)\n",
    "    tokenized_titles.append(tokenized_title)\n",
    "    tokenized_content = word_tokenize(article['content'])\n",
    "    len_before += len(tokenized_content)\n",
    "    tokenized_contents.append(tokenized_content)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T16:14:32.230415800Z",
     "start_time": "2024-04-04T16:14:29.478573500Z"
    }
   },
   "id": "e30dc2db26458201"
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T20:52:39.352442600Z",
     "start_time": "2024-04-02T20:52:38.793364500Z"
    }
   },
   "id": "b1ea6e30ecc5fceb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### StopWords removal"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "202f5682984480c6"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filtration:  1908\n",
      "after filtration:  1554\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words(\"arabic\"))\n",
    "filtred_titles = [word for title in tokenized_titles for word in title if not word in stopwords]\n",
    "filtred_content = [word for title in tokenized_contents for word in title if not word in stopwords]\n",
    "filtred_collection = filtred_titles + filtred_content\n",
    "print('before filtration: ',len_before)\n",
    "print('after filtration: ',len(filtred_collection))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T16:14:36.356724400Z",
     "start_time": "2024-04-04T16:14:36.296433600Z"
    }
   },
   "id": "21c8962463695f59"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Stemming using ISRIStemmer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f433e0417b48a8c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went from ميسي to يسي\n",
      "went from يواصل to يصل\n",
      "went from الغياب to غيب\n",
      "went from وإنتر to إنتر\n",
      "went from ميامي to يمي\n",
      "went from يخسر to خسر\n",
      "went from باريس to ارس\n",
      "went from سان to سان\n",
      "went from جيرمان to جيرم\n",
      "went from يعود to يعد\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import ISRIStemmer\n",
    "\n",
    "stemmer = ISRIStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtred_collection]\n",
    "for i in range(10):\n",
    "    print(f'went from {filtred_collection[i]} to {stemmed_words[i]}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T16:15:04.147799900Z",
     "start_time": "2024-04-04T16:15:04.108458200Z"
    }
   },
   "id": "3d4373d72508c48e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Lemmetization using Farasa API\n",
    "Farasa is the state-of-the-art full-stack package to deal with Arabic Language Processing. It has been developed by **Arabic Language Technologies Group** at **Qatar Computing Research Institute (QCRI)**. It has a RESTful Web API that you can use through your favorable programming language."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4ec33a02ec76ff0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "url = 'https://farasa.qcri.org/webapi/lemmatization/'\n",
    "t = 0\n",
    "api_key = \"vbnkEBqAmuLmFEjfqd\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb9e7a50ba238608"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "payload = {'text': ' '.join(filtred_collection), 'api_key': api_key}\n",
    "data = requests.post(url, data=payload)\n",
    "text = json.loads(data.text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e3cbafc888c4487"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went from ميسي to ميس\n",
      "went from يواصل to واصل\n",
      "went from الغياب to غياب\n",
      "went from وإنتر to إنتر\n",
      "went from ميامي to ميامي\n",
      "went from يخسر to خسر\n",
      "went from باريس to باريس\n",
      "went from سان to سان\n",
      "went from جيرمان to جيرمان\n",
      "went from يعود to عاد\n"
     ]
    }
   ],
   "source": [
    "lemmetized_words = text[\"text\"]\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'went from {filtred_collection[i]} to {lemmetized_words[i]}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T16:18:51.307093200Z",
     "start_time": "2024-04-04T16:18:51.235463900Z"
    }
   },
   "id": "5ac4e1a792f05c32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Comparison between Stemming and Lemmetization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8657fd22ce15360f"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1507,  1504  1507\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ls = ['(', '،', ')', '.', '/', ':', '؟', 'ب', '`', 'ل', \"'\", '؛'] \n",
    "lemmetized_words_f = [i for i in lemmetized_words if not i in ls]\n",
    "stemmed_words_f = [i for i in stemmed_words if not i in ls]\n",
    "filtred_collection_f = [i for i in filtred_collection if not i in ls]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T16:49:23.271330700Z",
     "start_time": "2024-04-04T16:49:23.254043300Z"
    }
   },
   "id": "c2d2bb184efae527"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "     original stemmed lemmetized\n0        ميسي     يسي        ميس\n1       يواصل     يصل       واصل\n2      الغياب     غيب       غياب\n3       وإنتر    إنتر       إنتر\n4       ميامي     يمي      ميامي\n...       ...     ...        ...\n1499  للتحقيق     حقق       تعلق\n1500    قضايا     قضا        سوء\n1501     فساد     فسد      إدارة\n1502    تتعلق     علق        غسل\n1503     بسوء     بسء        مال\n\n[1504 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>original</th>\n      <th>stemmed</th>\n      <th>lemmetized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ميسي</td>\n      <td>يسي</td>\n      <td>ميس</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>يواصل</td>\n      <td>يصل</td>\n      <td>واصل</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>الغياب</td>\n      <td>غيب</td>\n      <td>غياب</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>وإنتر</td>\n      <td>إنتر</td>\n      <td>إنتر</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ميامي</td>\n      <td>يمي</td>\n      <td>ميامي</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1499</th>\n      <td>للتحقيق</td>\n      <td>حقق</td>\n      <td>تعلق</td>\n    </tr>\n    <tr>\n      <th>1500</th>\n      <td>قضايا</td>\n      <td>قضا</td>\n      <td>سوء</td>\n    </tr>\n    <tr>\n      <th>1501</th>\n      <td>فساد</td>\n      <td>فسد</td>\n      <td>إدارة</td>\n    </tr>\n    <tr>\n      <th>1502</th>\n      <td>تتعلق</td>\n      <td>علق</td>\n      <td>غسل</td>\n    </tr>\n    <tr>\n      <th>1503</th>\n      <td>بسوء</td>\n      <td>بسء</td>\n      <td>مال</td>\n    </tr>\n  </tbody>\n</table>\n<p>1504 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data={'original':filtred_collection_f[:len(lemmetized_words_f)],'stemmed':stemmed_words_f[:len(lemmetized_words_f)],'lemmetized':lemmetized_words_f})\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T16:50:59.844167600Z",
     "start_time": "2024-04-04T16:50:59.799423600Z"
    }
   },
   "id": "9a1ac3449359d4fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### PoS tagging using Farasa PoS Tagger"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9936fcb0d03e7c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PoS_url = 'https://farasa.qcri.org/webapi/pos/'\n",
    "payload = {'text': \" \".join(lemmetized_words_f), 'api_key': api_key}\n",
    "data = requests.post(PoS_url, data=payload)\n",
    "result = json.loads(data.text)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29cef532ee719c63"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "         word     PoS_Tag\n0           S           S\n1         ميس        NOUN\n2        واصل           V\n3        غياب        NOUN\n4        إنتر        NOUN\n...       ...         ...\n1507      سوء        NOUN\n1508  إدار +ة  NOUN+NSUFF\n1509      غسل        NOUN\n1510      مال        NOUN\n1511        E           E\n\n[1512 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>PoS_Tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>S</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ميس</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>واصل</td>\n      <td>V</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>غياب</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>إنتر</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1507</th>\n      <td>سوء</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <th>1508</th>\n      <td>إدار +ة</td>\n      <td>NOUN+NSUFF</td>\n    </tr>\n    <tr>\n      <th>1509</th>\n      <td>غسل</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <th>1510</th>\n      <td>مال</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <th>1511</th>\n      <td>E</td>\n      <td>E</td>\n    </tr>\n  </tbody>\n</table>\n<p>1512 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df = pd.DataFrame({\"word\":[i['surface'] for i in result[\"text\"]],'PoS_Tag':[i['POS'] for i in result[\"text\"]]})\n",
    "pos_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T17:12:19.887698700Z",
     "start_time": "2024-04-04T17:12:19.860851500Z"
    }
   },
   "id": "49dcd81f264db649"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Named Entity Recognizer Using Farasa"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dd4a58746647332"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NER_url = 'https://farasa.qcri.org/webapi/ner/'\n",
    "payload = {'text': \" \".join(lemmetized_words_f), 'api_key': api_key}\n",
    "data = requests.post(NER_url, data=payload)\n",
    "result = json.loads(data.text)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51766c318d1ac14c"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "       word NER_Tag\n0       ميس       O\n1      واصل       O\n2      غياب       O\n3      إنتر   B-ORG\n4     ميامي   I-ORG\n...     ...     ...\n1499   تعلق       O\n1500    سوء       O\n1501  إدارة       O\n1502    غسل       O\n1503    مال       O\n\n[1504 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>NER_Tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ميس</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>واصل</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>غياب</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>إنتر</td>\n      <td>B-ORG</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ميامي</td>\n      <td>I-ORG</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1499</th>\n      <td>تعلق</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1500</th>\n      <td>سوء</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1501</th>\n      <td>إدارة</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1502</th>\n      <td>غسل</td>\n      <td>O</td>\n    </tr>\n    <tr>\n      <th>1503</th>\n      <td>مال</td>\n      <td>O</td>\n    </tr>\n  </tbody>\n</table>\n<p>1504 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_df = pd.DataFrame({\"word\":[i.split(sep='/')[0] for i in result[\"text\"]],'NER_Tag':[i.split(sep='/')[1] for i in result[\"text\"]]})\n",
    "ner_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T17:25:46.991445700Z",
     "start_time": "2024-04-04T17:25:46.914848100Z"
    }
   },
   "id": "57d0bbdf7930c4d9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Conclution\n",
    "Throughout this NoteBook i have learned to use BeautifulSoup to scrap Data from a website, deal with the encoding of arabic, make simple CRUD operations using MongoDB in Python, establishing a good NLP pipeline that can deal with Arabic, research how the NLP is used to treat Arabic with different technics.\n",
    "to sum up everything that has been said the technics and researches that where made in this domain, although they are still not enough but giving the difficulty of the language and the lack of good and financed scientific research in this field its understandable, and we hope to continue making it better"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "947fca591fd9ae5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e4ec3a8b844be94a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
